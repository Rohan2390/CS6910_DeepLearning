{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Team members\n**1) CS21M024 -Joyojyoti Acharya** \n\n**2) CS21M075-  Vrushab Karia**\n","metadata":{"id":"MZMbGCw0Qxfg"}},{"cell_type":"markdown","source":"#  Q8) Generating english lyrics by Fine tuning GPT-2\n","metadata":{}},{"cell_type":"markdown","source":"Reference:- https://towardsdatascience.com/natural-language-generation-part-2-gpt-2-and-huggingface-f3acb35bc86a\n\nWe have refered this blog for fine tuning GPT-2.","metadata":{}},{"cell_type":"markdown","source":"**We are cloning the transformers github repository**","metadata":{}},{"cell_type":"code","source":"#Cloning the transformers github repository\n!git clone https://github.com/huggingface/transformers","metadata":{"id":"91rmSAUQVIUP","outputId":"c85ce926-b52a-4f4f-abee-e60698fd32f7","execution":{"iopub.status.busy":"2022-05-08T04:05:38.764844Z","iopub.execute_input":"2022-05-08T04:05:38.765420Z","iopub.status.idle":"2022-05-08T04:05:49.904527Z","shell.execute_reply.started":"2022-05-08T04:05:38.765387Z","shell.execute_reply":"2022-05-08T04:05:49.903551Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Checking the GPU allocated to us**","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"BPIgByLqmI81","outputId":"cd2f81eb-ad1f-481f-df61-cbdc2d6b2b94","execution":{"iopub.status.busy":"2022-05-08T04:05:49.906619Z","iopub.execute_input":"2022-05-08T04:05:49.906895Z","iopub.status.idle":"2022-05-08T04:05:50.735022Z","shell.execute_reply.started":"2022-05-08T04:05:49.906865Z","shell.execute_reply":"2022-05-08T04:05:50.734068Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**We are Changing the directory location to be in the examples folder and then install any requirements**","metadata":{"id":"XOTdb4rWv8YN"}},{"cell_type":"code","source":"import os\nos.chdir(\"transformers\")\nos.chdir(\"./examples/tensorflow/language-modeling\")\n!pip install -r requirements.txt\n!pip install pyarrow --upgrade\n# Need to install latest transformer packages from github so the scripts will run correctly\n! pip install git+\"https://github.com/huggingface/transformers/\"","metadata":{"id":"K2M2Oz9CYB4P","outputId":"b50a1058-c92d-42d7-de0b-2f6f6b190ebc","execution":{"iopub.status.busy":"2022-05-08T04:05:50.736940Z","iopub.execute_input":"2022-05-08T04:05:50.737217Z","iopub.status.idle":"2022-05-08T04:07:01.118284Z","shell.execute_reply.started":"2022-05-08T04:05:50.737181Z","shell.execute_reply":"2022-05-08T04:07:01.117440Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Use this if your using the google collab notebook\n# from google.colab import drive \n# drive.mount('/content/gdrive')","metadata":{"id":"k7g3mGFNOHXd","outputId":"943dea2f-4313-4c2a-a7da-a0b58a237080","execution":{"iopub.status.busy":"2022-05-08T04:07:01.121767Z","iopub.execute_input":"2022-05-08T04:07:01.121993Z","iopub.status.idle":"2022-05-08T04:07:01.128504Z","shell.execute_reply.started":"2022-05-08T04:07:01.121966Z","shell.execute_reply":"2022-05-08T04:07:01.127213Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing the input data**","metadata":{"id":"oDuJnVmrY_Fb"}},{"cell_type":"markdown","source":"**We runned the below script in the local machine to concatenate all the lyrics file into one merged file. We have used the dataset 2 given in the question and merged all the files in that dataset.**","metadata":{}},{"cell_type":"code","source":"# import os\n# # find all the txt files in the dataset folder\n# inputs = []\n# for file in os.listdir(\"archive\"):\n#     if file.endswith(\".txt\"):\n#         inputs.append(os.path.join(\"archive\", file))\n \n \n# # concatanate all txt files in a file called merged_file.txt\n# with open('merged_file.txt', 'w',encoding=\"utf-8\") as outfile:\n#     for fname in inputs:\n#         with open(fname, encoding=\"utf-8\", errors='ignore') as infile:\n#             outfile.write(infile.read())\n# print(\"done\")","metadata":{"execution":{"iopub.status.busy":"2022-05-08T04:07:01.132171Z","iopub.execute_input":"2022-05-08T04:07:01.132355Z","iopub.status.idle":"2022-05-08T04:07:01.145452Z","shell.execute_reply.started":"2022-05-08T04:07:01.132332Z","shell.execute_reply":"2022-05-08T04:07:01.144567Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nloading the data line by line\n\"\"\"\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n#Please add the location of the merged_file.txt dataset here\nwith open('/kaggle/input/merged-file/merged_file.txt', 'r') as data:\n  dataset = [\"\" + x.strip() for x in data.readlines()]\n\ntrain, evalu = train_test_split(dataset, train_size=.9, random_state=2020)\nprint(\"training size:\" + str(len(train)))\nprint(\"Evaluation size: \" + str(len(evalu)))\n\nwith open('/kaggle/working/train_tmp.txt', 'w') as file_handle:\n  file_handle.write(\" \".join(train))\n\nwith open('/kaggle/working/eval_tmp.txt', 'w') as file_handle:\n  file_handle.write(\" \".join(evalu))","metadata":{"id":"uU4ckPTf9T-2","outputId":"f92280ab-e021-42d4-de76-a2991c1e821e","execution":{"iopub.status.busy":"2022-05-08T04:07:01.146980Z","iopub.execute_input":"2022-05-08T04:07:01.147275Z","iopub.status.idle":"2022-05-08T04:07:02.535414Z","shell.execute_reply.started":"2022-05-08T04:07:01.147234Z","shell.execute_reply":"2022-05-08T04:07:02.534618Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"1TYi8Oqs6Npo"}},{"cell_type":"markdown","source":"# **Fine tunning the GPT-2 Model**","metadata":{}},{"cell_type":"markdown","source":"**Note :- We have used 'distil GPT-2' which is a smaller version of the 'GPT-2' model instead of normal GPT2. This is because the original GPT-2 model is very huge and we could not load and train it given the computing resource we had. So due to the resource constraints we have used distil GPT-2. However it is advisable to use original GPT-2 if you have no resource constraints**","metadata":{}},{"cell_type":"code","source":"!python run_clm.py \\\n--model_type distilgpt2 \\\n--model_name_or_path distilgpt2\\\n--train_file \"/kaggle/working/train_tmp.txt\" \\\n--do_train \\\n--validation_file \"/kaggle/working/eval_tmp.txt\" \\\n--do_eval \\\n--per_gpu_train_batch_size 1 \\\n--save_steps -1 \\\n--num_train_epochs 5 \\\n--fp16 \\\n--output_dir=\"/kaggle/working/question_8_op\"","metadata":{"id":"yyV_rTL3ZE_H","outputId":"fc7f4cee-8c94-403b-e719-fc3686f38798","execution":{"iopub.status.busy":"2022-05-08T04:07:02.539280Z","iopub.execute_input":"2022-05-08T04:07:02.541164Z","iopub.status.idle":"2022-05-08T04:24:27.434341Z","shell.execute_reply.started":"2022-05-08T04:07:02.541123Z","shell.execute_reply":"2022-05-08T04:24:27.433472Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **Now lets use the fine tunned model**.","metadata":{"id":"3CpzI5mU1jPl"}},{"cell_type":"code","source":"# setup imports to use the model\nfrom transformers import TFGPT2LMHeadModel\nfrom transformers import GPT2Tokenizer\n\nmodel = TFGPT2LMHeadModel.from_pretrained(\"/kaggle/working/question_8_op\", from_pt=False)\ntokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n","metadata":{"id":"kFOx9AUa1tnk","execution":{"iopub.status.busy":"2022-05-08T04:24:27.436267Z","iopub.execute_input":"2022-05-08T04:24:27.436570Z","iopub.status.idle":"2022-05-08T04:24:36.618784Z","shell.execute_reply.started":"2022-05-08T04:24:27.436518Z","shell.execute_reply":"2022-05-08T04:24:36.618020Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Passing and Encoding the sample text ","metadata":{"id":"3oWWo4HJ4wd-"}},{"cell_type":"code","source":"s=\"I love deep learning,\" # This is the initial prompt to the model\ninput_ids = tokenizer.encode(s, return_tensors='tf')","metadata":{"id":"6xT0tc07_-SL","execution":{"iopub.status.busy":"2022-05-08T04:24:36.620011Z","iopub.execute_input":"2022-05-08T04:24:36.620261Z","iopub.status.idle":"2022-05-08T04:24:36.627147Z","shell.execute_reply.started":"2022-05-08T04:24:36.620228Z","shell.execute_reply":"2022-05-08T04:24:36.626163Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#  **Generating the lyrics from our input smaple**. \nReference used for parameter training https://huggingface.co/blog/how-to-generate\n\nNote:- You can play around with the paramters to get some better outputs.","metadata":{"id":"BZ3YNTNlBsh8"}},{"cell_type":"code","source":"generated_text_samples = model.generate(\n    input_ids, \n    max_length=200,  \n    num_return_sequences=5,\n    #no_repeat_ngram_size=2,\n    repetition_penalty=1.5,\n    top_p=0.92,\n    temperature=.85,\n    do_sample=True,\n    top_k=125,\n    early_stopping=True\n)","metadata":{"id":"NbzHNvvaAPns","execution":{"iopub.status.busy":"2022-05-08T04:24:36.629941Z","iopub.execute_input":"2022-05-08T04:24:36.630284Z","iopub.status.idle":"2022-05-08T04:24:49.027374Z","shell.execute_reply.started":"2022-05-08T04:24:36.630244Z","shell.execute_reply":"2022-05-08T04:24:49.026504Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# **Printing the lyrics**","metadata":{}},{"cell_type":"code","source":"#Print output for each sequence generated above\nfor i, beam in enumerate(generated_text_samples):\n  print(\"{}: {}\".format(i,tokenizer.decode(beam, skip_special_tokens=True)))\n  print()","metadata":{"id":"XPdteSR_B3w1","execution":{"iopub.status.busy":"2022-05-08T04:24:49.028606Z","iopub.execute_input":"2022-05-08T04:24:49.028848Z","iopub.status.idle":"2022-05-08T04:24:49.064820Z","shell.execute_reply.started":"2022-05-08T04:24:49.028817Z","shell.execute_reply":"2022-05-08T04:24:49.064148Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# **Thankyou**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"id":"GekD8zzvCq3b"}}]}